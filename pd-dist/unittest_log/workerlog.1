W0221 04:25:12.382736  6290 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 04:25:12.383682  6290 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
W0221 04:25:15.113438  6290 dist_tensor.cc:52] WARNING: Tensor dim 1 is already sharded on mesh dim0. Sharding a tensor dim with multiple mesh dim is not supported yet.
dist_attr is {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [0,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
I0221 04:25:15.114795  6290 tcp_utils.cc:134] Successfully connected to 10.44.14.13:60388
dist_attr is {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [-1,0], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
placements is [Replicate(), Replicate()]
W0221 04:36:10.308148  8408 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 04:36:10.309017  8408 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=False,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
W0221 04:36:13.074162  8408 dist_tensor.cc:52] WARNING: Tensor dim 1 is already sharded on mesh dim0. Sharding a tensor dim with multiple mesh dim is not supported yet.
dist_attr is {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [0,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
I0221 04:36:13.075762  8408 tcp_utils.cc:111] Retry to connect to 10.44.14.13:52103 while the server is not yet listening.
I0221 04:36:16.076026  8408 tcp_utils.cc:134] Successfully connected to 10.44.14.13:52103
dist_attr is {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [-1,0], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
placements is [Replicate(), Replicate()]
W0221 04:36:30.332243  8742 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 04:36:30.333156  8742 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=False,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
W0221 04:36:33.262058  8742 dist_tensor.cc:52] WARNING: Tensor dim 1 is already sharded on mesh dim0. Sharding a tensor dim with multiple mesh dim is not supported yet.
W0221 04:39:10.195173  9809 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 04:39:10.196101  9809 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
placements is [Replicate(), Replicate()]
W0221 05:11:34.171443 12842 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:11:34.172428 12842 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
dist_attr is {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after shard,dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
placements is [Replicate(), Replicate()]
dist_attr is {process_mesh: {shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after reshard,dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
W0221 05:27:21.270557 13830 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:27:21.271522 13830 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
dist_attr is {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after shard,dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
placements is [Replicate(), Replicate()]
dist_attr is {process_mesh: {shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after reshard,dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
W0221 05:28:11.699862 14117 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:28:11.700883 14117 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
dist_attr is {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after shard,
======================== dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
placements is [Replicate(), Replicate()]
dist_attr is {process_mesh: {shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}
after reshard,
======================== dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
W0221 05:34:26.578322 14675 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:34:26.579180 14675 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
after shard,
======================== dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
调用moe_utils._dist_reshape!
after reshard,
======================== dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
W0221 05:37:43.843951 15298 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:37:43.844856 15298 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
Traceback (most recent call last):
  File "semi_auto_parallel_moe_utils.py", line 131, in <module>
    TestMoEUtils().run_test_case()
  File "semi_auto_parallel_moe_utils.py", line 127, in run_test_case
    self.test_reshard_mesh_shape()
  File "semi_auto_parallel_moe_utils.py", line 114, in test_reshard_mesh_shape
    print(f"after shard,\n======================== dist_x is {dist_x}\n========================")
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 958, in __format__
    return object.__format__(self, format_spec)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 952, in __str__
    return tensor_to_string(self)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/tensor/to_string.py", line 458, in tensor_to_string
    return dist_tensor_to_string(tensor, prefix)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/tensor/to_string.py", line 425, in dist_tensor_to_string
    global_tensor = reshard(tensor, tensor.process_mesh, placements)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/distributed/auto_parallel/api.py", line 851, in reshard
    if _reshard_mesh_shape(dist_tensor, mesh, placements): #现在没有走到这个分支里
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 1005, in __bool__
    return self.__nonzero__()
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 993, in __nonzero__
    assert (
AssertionError: When Variable is used as the condition of if/while , Variable can only contain one element.
W0221 05:41:00.706715 15702 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:41:00.707664 15702 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
Traceback (most recent call last):
  File "semi_auto_parallel_moe_utils.py", line 131, in <module>
    TestMoEUtils().run_test_case()
  File "semi_auto_parallel_moe_utils.py", line 127, in run_test_case
    self.test_reshard_mesh_shape()
  File "semi_auto_parallel_moe_utils.py", line 114, in test_reshard_mesh_shape
    print(f"after shard,\n======================== dist_x is {dist_x}\n========================")
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 958, in __format__
    return object.__format__(self, format_spec)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/base/dygraph/tensor_patch_methods.py", line 952, in __str__
    return tensor_to_string(self)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/tensor/to_string.py", line 458, in tensor_to_string
    return dist_tensor_to_string(tensor, prefix)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/tensor/to_string.py", line 425, in dist_tensor_to_string
    global_tensor = reshard(tensor, tensor.process_mesh, placements)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/distributed/auto_parallel/api.py", line 832, in reshard
    if _only_reshard_mesh_shape(dist_tensor, mesh, placements):
NameError: name '_only_reshard_mesh_shape' is not defined
W0221 05:42:47.140688 16103 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 05:42:47.141752 16103 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
after shard,
======================== dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
after reshard,
======================== dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [1,2], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Replicate()], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
