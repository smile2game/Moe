WARNING: Logging before InitGoogleLogging() is written to STDERR
I0305 05:24:42.844835 12682 pybind.cc:703] pylayer has no kernels, skip
I0305 05:24:42.844933 12682 pybind.cc:703] while has no kernels, skip
I0305 05:24:42.844962 12682 pybind.cc:703] conditional_block has no kernels, skip
I0305 05:24:42.844971 12682 pybind.cc:703] py_func has no kernels, skip
I0305 05:24:42.875746 12682 dynamic_loader.cc:175] Set paddle lib path : /home/aistudio/.local/lib/python3.8/site-packages/paddle/libs
I0305 05:24:42.914068 12682 global_utils.h:89] Set current tracer for Controller: 0
I0305 05:24:42.914134 12682 tracer.cc:86] Set current tracer: 0
I0305 05:24:43.441167 12682 init.cc:101] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=enable_record_memory,enable_auto_layout_pass,prim_check_ops,enable_exit_when_partial_worker,benchmark_nccl,logging_pir_py_code_dir,fast_eager_deletion_mode,enable_cublas_tensor_op_math,graph_embedding_split_infer_mode,nccl_dir,gpugraph_debug_gpu_memory,dump_chunk_info,gpugraph_hbm_table_load_factor,enable_dump_main_program,enable_auto_parallel_align_mode,gpugraph_slot_feasign_max_num,cudnn_dir,set_to_1d,use_shm_cache,allocator_strategy,prim_enable_dynamic,cusparse_dir,accuracy_check_rtol_fp16,nvidia_package_dir,new_executor_static_build,prim_forward,enable_tracker_all2all,gpugraph_dedup_pull_push_mode,nccl_blocking_wait,local_exe_sub_scope_limit,query_dest_rank_by_multi_node,disable_dyshape_in_train,low_precision_op_list,use_mkldnn,accuracy_check_rtol_fp32,disable_logging_op_attr_list,inner_op_parallelism,print_sub_graph_dir,prim_skip_dynamic,prim_backward,use_system_allocator,lapack_dir,search_cache_max_number,prim_forward_blacklist,gpugraph_parallel_stream_num,eager_delete_tensor_gb,enable_neighbor_list_use_uva,memory_fraction_of_eager_deletion,gpugraph_offload_gather_copy_maxsize,embedding_deterministic,gpu_allocator_retry_time,enable_reuse_iters_in_fusion,init_allocated_mem,initial_gpu_memory_in_mb,tracer_onednn_ops_on,new_executor_use_local_scope,use_cuda_malloc_async_allocator,dynamic_static_unified_comm,cusolver_dir,gpugraph_offload_param_stat,pir_broadcast_tree_limit,all_blocks_convert_trt,cudnn_exhaustive_search,tracer_profile_fname,dataloader_use_file_descriptor,pir_apply_inplace_pass,gpugraph_offload_param_extends,cusparselt_dir,enable_pir_in_executor,enable_api_kernel_fallback,enable_gpu_memory_usage_log_mb,check_infer_symbolic,multiple_of_cupti_buffer_size,op_dir,gpugraph_storage_mode,logging_pir_py_code_int_tensor_element_limit,curand_dir,fused_multi_transformer_op_use_mbfmha,log_memory_stats,tcp_max_syn_backlog,print_allocator_trace_info,enable_auto_detect_gpu_topo,eager_communication_connection,apply_pass_to_program,allreduce_record_one_event,enable_dependency_builder_debug_info,jit_engine_type,use_xqa_optim,fraction_of_cpu_memory_to_use,cudnn_exhaustive_search_times,cudnn_batchnorm_spatial_persistent,new_executor_use_cuda_graph,multi_block_attention_min_partition_size,einsum_opt,eager_delete_scope,check_nan_inf_level,flash_attn_version,static_runtime_data_save_path,use_virtual_memory_auto_growth,communicator_max_merge_var_num,conv_workspace_size_limit,check_nan_inf,free_idle_chunk,accuracy_check_atol_fp16,benchmark,enable_append_iters_in_fusion,cache_inference_while_scope,gpugraph_enable_segment_merge_grads,enable_pir_api,initial_cpu_memory_in_mb,add_dependency_for_communication_op,prim_all,tracer_onednn_ops_off,enable_opt_get_features,host_trace_level,paddle_num_threads,gpugraph_parallel_copyer_split_maxsize,dygraph_debug,gpu_memory_limit_mb,get_host_by_name_time,selected_gpus,use_stride_kernel,conv2d_disable_cudnn,async_trace_count,enable_blaslt_global_search,new_executor_use_inplace,use_fast_math,alloc_fill_value,fraction_of_gpu_memory_to_use,enable_gpu_memory_usage_log,use_cuda_managed_memory,logging_trunc_pir_py_code,npu_storage_format,trt_min_group_size,gpugraph_sparse_table_storage_mode,executor_log_deps_every_microseconds,sync_nccl_allreduce,enable_custom_engine,tensor_operants_mode,mklml_dir,cuda_malloc_async_pool_memory_throttle_ratio,accuracy_check_atol_bf16,enable_all2all_use_fp16,dist_threadpool_size,pir_apply_shape_optimization_pass,enable_pir_with_pt_in_dy2st,check_kernel_launch,prim_enabled,mkl_dir,reader_queue_speed_test_mode,gpugraph_force_device_batch_num_equal,enable_sparse_inner_gather,cse_max_count,run_kp_kernel,graph_load_in_parallel,enable_cinn_accuracy_check,gpugraph_enable_print_op_debug,enable_fuse_parallel_matmul_pass,auto_free_cudagraph_allocations_on_launch,communicator_send_queue_size,cublas_dir,convert_all_blocks,graph_metapath_split_opt,enable_fusion_fallback,cublaslt_device_best_config,enable_async_trace,static_executor_perfstat_filepath,sync_after_alloc,gpugraph_enable_hbm_table_collision_stat,gpugraph_load_node_list_into_hbm,cudnn_deterministic,auto_growth_chunk_size_in_mb,fuse_parameter_groups_size,print_ir,enable_auto_rdma_trans,cuda_memory_async_pool_release_threshold,enable_adjust_op_order,gpugraph_merge_grads_segment_size,cupti_dir,use_auto_growth_v2,new_executor_sequential_run,enable_cse_in_dy2st,pir_interpreter_record_stream_for_gc_cache,enable_pir_in_executor_trace_run,use_stream_safe_cuda_allocator,enable_unused_var_check,cuda_core_int8_gemm,tensorrt_dir,trt_ibuilder_cache,accuracy_check_rtol_bf16,new_executor_serial_run,accuracy_check_atol_fp32,use_auto_growth_pinned_allocator,win_cuda_bin_dir,max_inplace_grad_add,graph_neighbor_size_percent,save_static_runtime_data,enable_graph_multi_node_sampling,enable_fusion_result_check,manually_trans_conv_filter,print_kernel_run_info,enable_transpose_iters_in_fusion,pinned_memory_as_cpu_backend,gpugraph_enable_gpu_direct_access,cuda_dir,graph_get_neighbor_id,ir_inplace_kernel_blacklist,save_cf_stack_op,use_autotune,force_sync_ops,gemm_use_half_precision_compute_type,sort_sum_gradient,cublaslt_exhaustive_search_times,communicator_is_sgd_optimizer,multi_node_sample_use_gpu_table,reallocate_gpu_memory_in_mb,free_when_no_cache_hit,logging_pir_py_code_dump_symbolic_dims,enable_collect_shape,call_stack_level,fraction_of_cuda_pinned_memory_to_use,use_pinned_memory,fuse_parameter_memory_size 
I0305 05:24:43.441334 12682 init.cc:109] After Parse: argc is 2
I0305 05:24:43.441584 12682 os_info.cc:117] SetCurrentThreadName MainThread
I0305 05:24:43.442195 12682 pybind.cc:2509] Initialize tensor operants successfully
I0305 05:24:43.555073 12682 amp_auto_cast.cc:113] -- The size of all_ops: 1146 --
I0305 05:24:43.555130 12682 amp_auto_cast.cc:114] -- The size of supported_ops: 208 --
I0305 05:24:43.555135 12682 amp_auto_cast.cc:115] -- The size of unsupported_ops: 938 --
I0305 05:24:44.007748 12682 global_utils.h:89] Set current tracer for Controller: 0x6076c10
I0305 05:24:44.007818 12682 tracer.cc:86] Set current tracer: 0x6076c10
I0305 05:24:44.007894 12682 imperative.cc:699] Tracer(0x6076c10) set expected place Place(gpu:1)
I0305 05:24:44.008044 12682 global_value_getter_setter.cc:189] set FLAGS_enable_pir_in_executor to True
**********shard_test************
I0305 05:24:44.024099 12682 ops_api.cc:1938] Call eager_api_full
I0305 05:24:44.024142 12682 eager_op_function.cc:16512] Running Eager Final State API: full
I0305 05:24:44.024175 12682 eager_utils.cc:2316] type_name: int
I0305 05:24:44.024195 12682 dygraph_functions.cc:34583] Running AD API: full
I0305 05:24:44.024200 12682 dygraph_functions.cc:34593]  No Type Promotion for full_ad_func api. 
I0305 05:24:44.024204 12682 dygraph_functions.cc:34596]  No Type Autocast for full_ad_func api. 
I0305 05:24:44.024209 12682 dygraph_functions.cc:34599] Running C++ API: full
I0305 05:24:44.024214 12682 dygraph_functions.cc:34607] { Input: []} 
I0305 05:24:44.024411 12682 api.cc:47814] full API kernel key: [CPU, Undefined(AnyLayout), int64]
I0305 05:24:44.024442 12682 api.cc:47821] full kernel: {"input":[],"output":["CPU, NCHW, int64"],"attribute":["IntArray","Scalar","DataType"]}
I0305 05:24:44.024464 12682 context_pool.cc:62] DeviceContextPool Get: Place(cpu)
I0305 05:24:44.024513 12682 allocator_facade.cc:212] selected allocator strategy:1
I0305 05:24:44.024539 12682 allocator_facade.cc:1051] FLAGS_auto_growth_chunk_size_in_mb is 0
I0305 05:24:44.024679 12682 dynamic_loader.cc:225] Try to find library: libcuda.so from default system path.
I0305 05:24:44.024713 12682 auto_growth_best_fit_allocator.cc:64] chunk_size_:256
I0305 05:24:44.024724 12682 allocator_facade.cc:1051] FLAGS_auto_growth_chunk_size_in_mb is 0
I0305 05:24:44.024729 12682 auto_growth_best_fit_allocator.cc:64] chunk_size_:256
I0305 05:24:44.025115 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 1 Allocator = 0x245b3f0
I0305 05:24:44.025156 12682 generator.cc:181] Generator Random state device id: -1, seed: 2432613852819464, offset: 0, cpu_engine: 0x245b560
I0305 05:24:44.025168 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 1 Allocator = 0x245b3f0
I0305 05:24:44.025174 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 0 Allocator = 0x2451120
I0305 05:24:44.025180 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 0 Allocator = 0x2451120
I0305 05:24:44.025463 12682 dygraph_functions.cc:34623] Finish AD API: full
I0305 05:24:44.025802 12682 dygraph_functions.cc:34637] { Input: [],  
 Output: [ 
( out , [{Name: None, Initialized: 1, Ptr: 0x60b4930,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ None ]}]), ] } 
I0305 05:24:44.025933 12682 accumulation_node.h:30] Construct GradNodeAccumulation
I0305 05:24:44.026091 12682 ops_api.cc:1938] Call eager_api_full
I0305 05:24:44.026108 12682 eager_op_function.cc:16512] Running Eager Final State API: full
I0305 05:24:44.026116 12682 eager_utils.cc:2316] type_name: int
I0305 05:24:44.026126 12682 dygraph_functions.cc:34583] Running AD API: full
I0305 05:24:44.026130 12682 dygraph_functions.cc:34593]  No Type Promotion for full_ad_func api. 
I0305 05:24:44.026135 12682 dygraph_functions.cc:34596]  No Type Autocast for full_ad_func api. 
I0305 05:24:44.026139 12682 dygraph_functions.cc:34599] Running C++ API: full
I0305 05:24:44.026144 12682 dygraph_functions.cc:34607] { Input: []} 
I0305 05:24:44.026152 12682 api.cc:47814] full API kernel key: [CPU, Undefined(AnyLayout), int64]
I0305 05:24:44.026163 12682 api.cc:47821] full kernel: {"input":[],"output":["CPU, NCHW, int64"],"attribute":["IntArray","Scalar","DataType"]}
I0305 05:24:44.026192 12682 dygraph_functions.cc:34623] Finish AD API: full
I0305 05:24:44.026211 12682 dygraph_functions.cc:34637] { Input: [],  
 Output: [ 
( out , [{Name: None, Initialized: 1, Ptr: 0x61dad30,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ None ]}]), ] } 
I0305 05:24:44.026229 12682 accumulation_node.h:30] Construct GradNodeAccumulation
I0305 05:24:44.026294 12682 ops_api.cc:1938] Call eager_api_full
I0305 05:24:44.026306 12682 eager_op_function.cc:16512] Running Eager Final State API: full
I0305 05:24:44.026314 12682 eager_utils.cc:2316] type_name: int
I0305 05:24:44.026321 12682 dygraph_functions.cc:34583] Running AD API: full
I0305 05:24:44.026326 12682 dygraph_functions.cc:34593]  No Type Promotion for full_ad_func api. 
I0305 05:24:44.026331 12682 dygraph_functions.cc:34596]  No Type Autocast for full_ad_func api. 
I0305 05:24:44.026335 12682 dygraph_functions.cc:34599] Running C++ API: full
I0305 05:24:44.026340 12682 dygraph_functions.cc:34607] { Input: []} 
I0305 05:24:44.026345 12682 api.cc:47814] full API kernel key: [CPU, Undefined(AnyLayout), int64]
I0305 05:24:44.026352 12682 api.cc:47821] full kernel: {"input":[],"output":["CPU, NCHW, int64"],"attribute":["IntArray","Scalar","DataType"]}
I0305 05:24:44.026371 12682 dygraph_functions.cc:34623] Finish AD API: full
I0305 05:24:44.026383 12682 dygraph_functions.cc:34637] { Input: [],  
 Output: [ 
( out , [{Name: None, Initialized: 1, Ptr: 0x6092cb0,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ None ]}]), ] } 
I0305 05:24:44.026398 12682 accumulation_node.h:30] Construct GradNodeAccumulation
I0305 05:24:44.026423 12682 ops_api.cc:8415] Call eager_api_arange
I0305 05:24:44.026578 12682 eager_op_function.cc:42527] Running Eager Final State API: arange
I0305 05:24:44.026623 12682 cuda_info.cc:257] SetDeviceId 1
I0305 05:24:44.026633 12682 eager_op_function.cc:42555] CurrentDeviceId: 1 from 1
I0305 05:24:44.026823 12682 dygraph_functions.cc:86152] Running AD API: arange
I0305 05:24:44.026835 12682 dygraph_functions.cc:86177]  No Type Promotion for arange_ad_func api. 
I0305 05:24:44.026840 12682 dygraph_functions.cc:86180]  No Type Autocast for arange_ad_func api. 
I0305 05:24:44.026849 12682 dygraph_functions.cc:86202] Running C++ API: arange
I0305 05:24:44.027098 12682 dygraph_functions.cc:86219] { Input: [ 
( start , [{Name: None, Initialized: 1, Ptr: 0x60b4930,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]),  
( end , [{Name: None, Initialized: 1, Ptr: 0x61dad30,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]),  
( step , [{Name: None, Initialized: 1, Ptr: 0x6092cb0,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]), ]} 
I0305 05:24:44.027464 12682 api.cc:122522] arange API kernel key: [GPU, NCHW, int64]
I0305 05:24:44.027484 12682 api.cc:122529] arange_tensor kernel: {"input":["Undefined, NCHW, int64","Undefined, NCHW, int64","Undefined, NCHW, int64"],"output":["GPU, NCHW, int64"],"attribute":[]}
I0305 05:24:44.027495 12682 context_pool.cc:62] DeviceContextPool Get: Place(gpu:1)
W0305 05:24:44.027905 12682 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
I0305 05:24:44.028095 12682 dynamic_loader.cc:225] Try to find library: libcudnn.so from default system path.
W0305 05:24:44.028832 12682 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
I0305 05:24:46.755354 12682 allocator_facade.cc:382] [GetAllocator] place = Place(gpu:1) size = 1 Allocator = 0x245b450
I0305 05:24:46.755471 12682 allocator_facade.cc:382] [GetAllocator] place = Place(gpu_pinned) size = 1 Allocator = 0x245b480
I0305 05:24:46.755519 12682 generator.cc:181] Generator Random state device id: 1, seed: 216749716354847, offset: 0, cpu_engine: 0x4bb8dd20
I0305 05:24:46.755527 12682 generator.cc:87] initial seed: 216749716354847
I0305 05:24:46.755537 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 1 Allocator = 0x245b3f0
I0305 05:24:46.755543 12682 allocator_facade.cc:382] [GetAllocator] place = Place(gpu:1) size = 0 Allocator = 0x245b160
I0305 05:24:46.755549 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 0 Allocator = 0x2451120
I0305 05:24:46.756577 12682 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f4579400000), and remaining 0
I0305 05:24:46.756740 12682 dygraph_functions.cc:86235] Finish AD API: arange
I0305 05:24:46.756875 12682 dygraph_functions.cc:86258] { Input: [ 
( start , [{Name: None, Initialized: 1, Ptr: 0x60b4930,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]),  
( end , [{Name: None, Initialized: 1, Ptr: 0x61dad30,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]),  
( step , [{Name: None, Initialized: 1, Ptr: 0x6092cb0,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(cpu), Shape: 1, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ BackwardOutMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ], BackwardInMeta: [  {SlotSize: [1]: SlotID: 0, StopGradients: 0, , Edges[ { NULL Edge } ]}  ] ], StopGradient: [ 1 ] ]}]), ],  
 Output: [ 
( out , [{Name: None, Initialized: 1, Ptr: 0x4c1b4c50,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(gpu:1), Shape: 16, DistAttr: Unknown ], ADInfo:[ None ]}]), ] } 
I0305 05:24:46.756928 12682 accumulation_node.h:43] Destruct GradNodeAccumulation
I0305 05:24:46.756953 12682 accumulation_node.h:43] Destruct GradNodeAccumulation
I0305 05:24:46.756961 12682 accumulation_node.h:43] Destruct GradNodeAccumulation
I0305 05:24:46.757318 12682 layout_autotune.cc:31] Already exists in Layout OP: instance_norm
I0305 05:24:46.757515 12682 layout_autotune.cc:31] Already exists in Layout OP: transpose2
I0305 05:24:46.757668 12682 layout_autotune.cc:31] Already exists in Layout OP: reshape2
I0305 05:24:46.757867 12682 layout_autotune.cc:31] Already exists in Layout OP: batch_norm
I0305 05:24:46.758543 12682 layout_autotune.cc:31] Already exists in Layout OP: transpose
I0305 05:24:46.758780 12682 layout_autotune.cc:31] Already exists in Layout OP: softmax
I0305 05:24:46.758847 12682 layout_autotune.cc:83] The number of layout agnostic OPs: 546, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 140
I0305 05:24:46.758867 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 1
I0305 05:24:46.759071 12682 ops_api.cc:3738] Call eager_api_reshape
I0305 05:24:46.759250 12682 eager_op_function.cc:31953] Running Eager Final State API: reshape
I0305 05:24:46.759286 12682 eager_op_function.cc:31978] CurrentDeviceId: 1 from 1
I0305 05:24:46.759465 12682 dygraph_functions.cc:65200] Running AD API: reshape
I0305 05:24:46.759479 12682 dygraph_functions.cc:65223]  No Type Promotion for reshape_ad_func api. 
I0305 05:24:46.759483 12682 dygraph_functions.cc:65226]  No Type Autocast for reshape_ad_func api. 
I0305 05:24:46.759496 12682 dygraph_functions.cc:65249] Running C++ API: reshape
I0305 05:24:46.759545 12682 dygraph_functions.cc:65260] { Input: [ 
( x , [{Name: None, Initialized: 1, Ptr: 0x4c1b4c50,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(gpu:1), Shape: 16, DistAttr: Unknown ], ADInfo:[ None ]}]), ]} 
I0305 05:24:46.759727 12682 api.cc:92757] reshape API kernel key: [GPUDNN, NCHW, int64]
I0305 05:24:46.759747 12682 api.cc:92764] reshape kernel: {"input":["GPU, STRIDED, int64"],"output":["GPU, STRIDED, int64"],"attribute":["IntArray"]}
I0305 05:24:46.759771 12682 api.cc:92783] Perform View between Output and Input Tensor, share allocation and inplace version.
I0305 05:24:46.760129 12682 dygraph_functions.cc:65327] Finish AD API: reshape
I0305 05:24:46.760164 12682 dygraph_functions.cc:65341] { Input: [ 
( x , [{Name: None, Initialized: 1, Ptr: 0x4c1b4c50,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(gpu:1), Shape: 16, DistAttr: Unknown ], ADInfo:[ None ]}]), ],  
 Output: [ 
( out , [{Name: None, Initialized: 1, Ptr: 0x6092cb0,TensorInfo: [ Type: DenseTensor, Dtype: int64, Place: Place(gpu:1), Shape: 4, 4, DistAttr: Unknown ], ADInfo:[ Grad: [ {Name: None, Initialized: 0, Ptr: 0,TensorInfo: [ Unknown ], ADInfo:[ None ]} ],  GradNode: [ None ], StopGradient: [ 1 ] ]}]), ] } 
I0305 05:24:46.760221 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:46.760704 12682 eager_method.cc:368] Getting DenseTensor's numpy value
I0305 05:24:46.760751 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
I0305 05:24:46.760877 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:46.760900 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:46.761322 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
I0305 05:24:46.761484 12682 eager.cc:920]  args_num: 1
I0305 05:24:46.761499 12682 eager.cc:1056] Calling case5's or case6's or case7's initializer.
I0305 05:24:46.761546 12682 eager.cc:251] Do TensorCopy from DenseTensor to DistTensor.
W0305 05:24:46.761569 12682 dist_tensor.cc:52] WARNING: Tensor dim 0 is already sharded on mesh dim0. Sharding a tensor dim with multiple mesh dim is not supported yet.
I0305 05:24:46.761736 12682 dist_tensor.cc:219] Reshard tensor: reshard from {Global Shape: 4, 4, Local Shape: 4, 4, DistAttr: {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [], partial: [].}} to {DistAttr: {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}}
I0305 05:24:46.761813 12682 reshard_function_registry.cc:39] Choose ReshardFunction: SameNdMeshReshard
I0305 05:24:46.761824 12682 context_pool.cc:62] DeviceContextPool Get: Place(gpu:1)
I0305 05:24:46.761837 12682 nd_mesh_reshard_function.cc:94] Call SameNdMeshReshard
I0305 05:24:46.761852 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:46.761858 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] -1,-1
I0305 05:24:46.761864 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.761871 12682 nd_mesh_reshard_function.cc:231] Step4: out_mesh axis : 1; partial state :0
I0305 05:24:46.761879 12682 reshard_utils.cc:71] Searching current global rank 1 in process_mesh {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:46.761893 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [1], process_ids: [1], dim_names: [y]}
I0305 05:24:46.761904 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] -1,-1
I0305 05:24:46.761909 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.761916 12682 r_to_s_reshard_function.cc:48] Call RToSReshard
I0305 05:24:46.761921 12682 reshard_utils.cc:71] Searching current global rank 1 in process_mesh {shape: [1], process_ids: [1], dim_names: [y]}
I0305 05:24:46.761926 12682 r_to_s_reshard_function.cc:63] RToSReshard: Tensor will be split on axis 0. Split will use axis 0 of process_mesh. There will have 1 process participate in.
I0305 05:24:46.761946 12682 r_to_s_reshard_function.cc:80] Call `Split` in Resharding on device.
I0305 05:24:46.762534 12682 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f4579400200), and remaining 0
I0305 05:24:46.762583 12682 memcpy.cc:383] memory::Copy 128 Bytes from Place(gpu:1) to Place(gpu:1) by stream(0x7ca2df0)
I0305 05:24:46.762636 12682 r_to_s_reshard_function.cc:82] The current process will remain the idx 0 piece of tensor
I0305 05:24:46.762653 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [1], process_ids: [1], dim_names: [y]}
I0305 05:24:46.762662 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] 0,-1
I0305 05:24:46.762668 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.762683 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:46.762693 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] 1,-1
I0305 05:24:46.762698 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.762712 12682 eager.cc:257] Same place, do ShareDataWith for DistTensor.
I0305 05:24:46.762741 12682 accumulation_node.h:30] Construct GradNodeAccumulation
I0305 05:24:46.762779 12682 eager_method.cc:368] Getting DenseTensor's numpy value
I0305 05:24:46.762794 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
dist_x._local_value().numpy() is [[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]
I0305 05:24:46.763242 12682 eager.cc:920]  args_num: 1
I0305 05:24:46.763262 12682 eager.cc:1056] Calling case5's or case6's or case7's initializer.
I0305 05:24:46.763304 12682 eager.cc:251] Do TensorCopy from DenseTensor to DistTensor.
I0305 05:24:46.763346 12682 dist_tensor.cc:219] Reshard tensor: reshard from {Global Shape: 4, 4, Local Shape: 4, 4, DistAttr: {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [], partial: [].}} to {DistAttr: {process_mesh: {shape: [2], process_ids: [0,1], dim_names: [x]}, dims_mappings: [0,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}}
I0305 05:24:46.763388 12682 reshard_function_registry.cc:39] Choose ReshardFunction: RToSReshard
I0305 05:24:46.763394 12682 context_pool.cc:62] DeviceContextPool Get: Place(gpu:1)
I0305 05:24:46.763403 12682 r_to_s_reshard_function.cc:48] Call RToSReshard
I0305 05:24:46.763409 12682 reshard_utils.cc:71] Searching current global rank 1 in process_mesh {shape: [2], process_ids: [0,1], dim_names: [x]}
I0305 05:24:46.763417 12682 r_to_s_reshard_function.cc:63] RToSReshard: Tensor will be split on axis 0. Split will use axis 0 of process_mesh. There will have 2 process participate in.
I0305 05:24:46.763422 12682 r_to_s_reshard_function.cc:80] Call `Split` in Resharding on device.
I0305 05:24:46.763478 12682 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f4579400400), and remaining 0
I0305 05:24:46.763507 12682 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f4579400600), and remaining 0
I0305 05:24:46.763526 12682 memcpy.cc:383] memory::Copy 64 Bytes from Place(gpu:1) to Place(gpu:1) by stream(0x7ca2df0)
I0305 05:24:46.763562 12682 memcpy.cc:383] memory::Copy 64 Bytes from Place(gpu:1) to Place(gpu:1) by stream(0x7ca2df0)
I0305 05:24:46.763583 12682 r_to_s_reshard_function.cc:82] The current process will remain the idx 1 piece of tensor
I0305 05:24:46.763597 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [2], process_ids: [0,1], dim_names: [x]}
I0305 05:24:46.763609 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] 0,-1
I0305 05:24:46.763615 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.763634 12682 eager.cc:257] Same place, do ShareDataWith for DistTensor.
I0305 05:24:46.763664 12682 accumulation_node.h:30] Construct GradNodeAccumulation
I0305 05:24:46.763692 12682 eager_method.cc:368] Getting DenseTensor's numpy value
I0305 05:24:46.763705 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
dist_z._local_value().numpy() is [[ 8  9 10 11]
 [12 13 14 15]]
I0305 05:24:46.763983 12682 eager_method.cc:368] Getting DenseTensor's numpy value
I0305 05:24:46.763999 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
I0305 05:24:46.764041 12682 eager_method.cc:368] Getting DenseTensor's numpy value
I0305 05:24:46.764053 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
_local_value改变
I0305 05:24:46.764125 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
mesh is {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
placements is [Replicate(), Replicate()]
I0305 05:24:46.764305 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:46.764554 12682 reshard_fwd_func.cc:25] Running AD API: reshard dygraph
I0305 05:24:46.764588 12682 context_pool.cc:62] DeviceContextPool Get: Place(gpu:1)
I0305 05:24:46.764613 12682 tensor_utils.cc:209] Reshard func: tensor(generated_tensor_0) reshard from {Global Shape: 4, 4, Local Shape: 4, 4, DistAttr: {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [], partial: [].}} to {DistAttr: {process_mesh: {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, dims_mappings: [-1,-1], batch_dim: 0, chunk_id: 0, skip_check_mesh: 0, dynamic_dims: [0,0], annotated: [dims_mapping: 1,process_mesh: 1], partial: [].}}
I0305 05:24:46.764652 12682 reshard_function_registry.cc:39] Choose ReshardFunction: SameNdMeshReshard
I0305 05:24:46.764659 12682 nd_mesh_reshard_function.cc:94] Call SameNdMeshReshard
I0305 05:24:46.764672 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:46.764684 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] 1,-1
I0305 05:24:46.764690 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.764698 12682 nd_mesh_reshard_function.cc:152] Step2: in_mesh axis 1
I0305 05:24:46.764703 12682 reshard_utils.cc:71] Searching current global rank 1 in process_mesh {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:46.764721 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [1], process_ids: [1], dim_names: [y]}
I0305 05:24:46.764732 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] 0,-1
I0305 05:24:46.764736 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:46.764743 12682 s_to_r_reshard_function.cc:112] Call SToRReshard
I0305 05:24:46.764748 12682 s_to_r_reshard_function.cc:124] Balanced reshard from shard to replicated
I0305 05:24:46.764760 12682 reshard_utils.cc:99] local world size: 1 local rank: 0
I0305 05:24:46.764914 12682 tcp_utils.cc:111] Retry to connect to 10.44.14.29:50095 while the server is not yet listening.
I0305 05:24:49.765156 12682 tcp_utils.cc:134] Successfully connected to 10.44.14.29:50095
I0305 05:24:49.795258 12682 dynamic_loader.cc:225] Try to find library: libnccl.so from default system path.
I0305 05:24:50.034678 12682 comm_context_manager.cc:93] init NCCLCommContext rank: 0, size: 1, unique_comm_key: ReshardGroup/1, unique_key: NCCLCommContext/ReshardGroup/1, nccl_id: ede491ad874deb6720c2c5a2ce1d0000000000000000000000007073694b000040d02f4c000031c6929467f007073694b0000d82fd286fc7f0040d054c000007c8b744fa7be3d030d286fc7f000d02f4c0000b030d286fc7f0040d02f4c0000
I0305 05:24:50.091053 12682 s_to_r_reshard_function.cc:50] Call `AllGather` in Resharding on device.
I0305 05:24:50.091302 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [1], process_ids: [1], dim_names: [y]}
I0305 05:24:50.091336 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] -1,-1
I0305 05:24:50.091346 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:50.091377 12682 dist_attr.cc:159] [TensorDistAttr verify_process_mesh] {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
I0305 05:24:50.091385 12682 dist_attr.cc:174] [TensorDistAttr verify_dims_mapping] -1,-1
I0305 05:24:50.091389 12682 dist_attr.cc:238] [TensorDistAttr verify_partial_status] []
I0305 05:24:50.091537 12682 eager_method.cc:345] Getting DistTensor's numpy value
I0305 05:24:50.091562 12682 allocator_facade.cc:382] [GetAllocator] place = Place(cpu) size = 256 Allocator = 0x245b3f0
I0305 05:24:50.091674 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:50.091694 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:50.092083 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
shard后,
======================== dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:1), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Replicate(), Shard(dim=0)], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
mesh is shape [2], process_ids [0, 1], dim_names ['x']
placements is [Shard(dim=0)]
进入到判断当中
src_placements is [Replicate(), Shard(dim=0)],
src_mesh is {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
dst_placements is [Shard(dim=0)],
dst_mesh is shape [2], process_ids [0, 1], dim_names ['x']
src_mesh.shape[1] is 1
src_len is 2,dst_len is 1
only reshard mesh shape,走对了就是这条路!
I0305 05:24:50.092453 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:50.092469 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
I0305 05:24:50.092592 12682 eager_properties.cc:614] eager_properties 'Shape' method, layout autotune  desired_layout: Undefined(AnyLayout) default_layout: Undefined(AnyLayout) tensor layout: NCHW tensor's shape size is : 2
Traceback (most recent call last):
  File "semi_auto_parallel_moe_utils.py", line 222, in <module>
    TestMoEUtils().run_test_case()
  File "semi_auto_parallel_moe_utils.py", line 218, in run_test_case
    self.test_reshard_mesh_shape_shard()
  File "semi_auto_parallel_moe_utils.py", line 203, in test_reshard_mesh_shape_shard
    dist_y = dist.reshard(
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/distributed/auto_parallel/api.py", line 836, in reshard
    return _reshard_mesh_shape(dist_tensor, mesh, placements)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/distributed/auto_parallel/moe_utils.py", line 449, in _reshard_mesh_shape
    return _dist_reshape(dist_tensor, dist_tensor.shape, mesh, dst_placements)
  File "/home/aistudio/.local/lib/python3.8/site-packages/paddle/distributed/auto_parallel/moe_utils.py", line 342, in _dist_reshape
    assert np.prod(tgt_local_shape) == np.prod(
AssertionError: The local shapes [4, 4] and [2, 4] are mismatched.
I0305 05:24:50.094647 12682 imperative.cc:699] Tracer(0x6076c10) set expected place Place(gpu:1)
I0305 05:24:50.094712 12682 global_utils.h:89] Set current tracer for Controller: 0
I0305 05:24:50.094724 12682 tracer.cc:86] Set current tracer: 0
I0305 05:24:50.094784 12682 mmap_allocator.cc:346] PID: 12682, MemoryMapFdSet: set size - 0
I0305 05:24:50.103771 12682 mmap_allocator.cc:346] PID: 12682, MemoryMapFdSet: set size - 0
I0305 05:24:50.155516 12682 accumulation_node.h:43] Destruct GradNodeAccumulation
I0305 05:24:50.155607 12682 accumulation_node.h:43] Destruct GradNodeAccumulation
I0305 05:24:50.291967 12682 mmap_allocator.cc:346] PID: 12682, MemoryMapFdSet: set size - 0
