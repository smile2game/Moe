**********shard_test************
W0221 10:15:26.303407 40681 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0221 10:15:26.304277 40681 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
before shard,
======================== x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:0), stop_gradient=True,
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
dist_x._local_value().numpy() is [[0 1 2 3]
 [4 5 6 7]]
dist_z._local_value().numpy() is [[0 1 2 3]
 [4 5 6 7]]
_local_value不变
mesh is {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
placements is [Replicate(), Replicate()]
I0221 10:15:28.972242 40681 tcp_utils.cc:185] The server starts to listen on IP_ANY:45254; setting synclog to 2048
I0221 10:15:28.972488 40681 tcp_utils.cc:134] Successfully connected to 10.44.14.13:45254
after shard,
======================== dist_x is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:0), stop_gradient=True, process_mesh={shape: [2,1], process_ids: [0,1], dim_names: [x,y]}, placements=[Shard(dim=0), Shard(dim=1)], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
mesh is shape [2], process_ids [0, 1], dim_names ['x']
placements is [Shard(dim=0)]
进入到判断当中
src_placements is [Shard(dim=0), Shard(dim=1)],
src_mesh is {shape: [2,1], process_ids: [0,1], dim_names: [x,y]}
dst_placements is [Shard(dim=0)],
dst_mesh is shape [2], process_ids [0, 1], dim_names ['x']
src_mesh.shape[1] is 1
src_len is 2,dst_len is 1
only reshard mesh shape,走对了就是这条路!
mesh is {shape: [2], process_ids: [0,1], dim_names: [x]}
placements is [Replicate()]
after reshard,
======================== dist_y is Tensor(shape=[4, 4], dtype=int64, place=Place(gpu:0), stop_gradient=True, process_mesh={shape: [2], process_ids: [0,1], dim_names: [x]}, placements=[Shard(dim=0)], GlobalDenseTensor=
       [[0 , 1 , 2 , 3 ],
        [4 , 5 , 6 , 7 ],
        [8 , 9 , 10, 11],
        [12, 13, 14, 15]])
========================
dist_y._local_value().numpy() is [[0 1 2 3]
 [4 5 6 7]]
 dist_x._local_value().numpy() is [[0 1 2 3]
 [4 5 6 7]]
I0221 10:15:29.568401 40748 tcp_store.cc:292] receive shutdown event and so quit from MasterDaemon run loop
